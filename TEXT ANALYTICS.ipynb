{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94619a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=''' Machine Learning can play a pivotal role in a range of applications such as Deep Learning, Reinforcement Learning, Natural Language Processing, etc. A prime example of the application of machine learning is the autonomous vehicle. Sensors around the vehicle deliver thousands of data points which are analyzed and processed to move the vehicle toward its destination. Collective data from thousands of self-driving cars can be used to improve vehicle safety and prevent accidents.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808cd2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c0772",
   "metadata": {},
   "source": [
    "### LOADING NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df9ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from NLTK) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from NLTK) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from NLTK) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from NLTK) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from click->NLTK) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cac756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa955a",
   "metadata": {},
   "source": [
    "### TOKENIZING THE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa02014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " machine learning can play a pivotal role in a range of applications such as deep learning, reinforcement learning, natural language processing, etc. a prime example of the application of machine learning is the autonomous vehicle. sensors around the vehicle deliver thousands of data points which are analyzed and processed to move the vehicle toward its destination. collective data from thousands of self-driving cars can be used to improve vehicle safety and prevent accidents.\n"
     ]
    }
   ],
   "source": [
    "#convert to lowercase\n",
    "lower=paragraph\n",
    "print(paragraph.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7578276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'Learning',\n",
       " 'can',\n",
       " 'play',\n",
       " 'a',\n",
       " 'pivotal',\n",
       " 'role',\n",
       " 'in',\n",
       " 'a',\n",
       " 'range',\n",
       " 'of',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Deep',\n",
       " 'Learning,',\n",
       " 'Reinforcement',\n",
       " 'Learning,',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing,',\n",
       " 'etc.',\n",
       " 'A',\n",
       " 'prime',\n",
       " 'example',\n",
       " 'of',\n",
       " 'the',\n",
       " 'application',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'autonomous',\n",
       " 'vehicle.',\n",
       " 'Sensors',\n",
       " 'around',\n",
       " 'the',\n",
       " 'vehicle',\n",
       " 'deliver',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'data',\n",
       " 'points',\n",
       " 'which',\n",
       " 'are',\n",
       " 'analyzed',\n",
       " 'and',\n",
       " 'processed',\n",
       " 'to',\n",
       " 'move',\n",
       " 'the',\n",
       " 'vehicle',\n",
       " 'toward',\n",
       " 'its',\n",
       " 'destination.',\n",
       " 'Collective',\n",
       " 'data',\n",
       " 'from',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'self-driving',\n",
       " 'cars',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'vehicle',\n",
       " 'safety',\n",
       " 'and',\n",
       " 'prevent',\n",
       " 'accidents.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split it into wprds--->words are called TOKENS\n",
    "paragraph.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1c662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'Learning', 'can', 'play', 'a', 'pivotal', 'role', 'in', 'a', 'range', 'of', 'applications', 'such', 'as', 'Deep', 'Learning', ',', 'Reinforcement', 'Learning', ',', 'Natural', 'Language', 'Processing', ',', 'etc', '.', 'A', 'prime', 'example', 'of', 'the', 'application', 'of', 'machine', 'learning', 'is', 'the', 'autonomous', 'vehicle', '.', 'Sensors', 'around', 'the', 'vehicle', 'deliver', 'thousands', 'of', 'data', 'points', 'which', 'are', 'analyzed', 'and', 'processed', 'to', 'move', 'the', 'vehicle', 'toward', 'its', 'destination', '.', 'Collective', 'data', 'from', 'thousands', 'of', 'self-driving', 'cars', 'can', 'be', 'used', 'to', 'improve', 'vehicle', 'safety', 'and', 'prevent', 'accidents', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token=word_tokenize(paragraph)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3677ff",
   "metadata": {},
   "source": [
    "### FREQUENCY DISTINCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca84bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'of': 5, '.': 4, 'the': 4, 'vehicle': 4, 'Learning': 3, ',': 3, 'can': 2, 'a': 2, 'thousands': 2, 'data': 2, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7142481",
   "metadata": {},
   "source": [
    "### MOST COMMON---.MOST COMMON WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438d9b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 5), ('.', 4), ('the', 4), ('vehicle', 4), ('Learning', 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1=fdist.most_common(5)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f1df4",
   "metadata": {},
   "source": [
    "### N-GRAMS-->FINDING RELATIVE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c2e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dharniha v\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50cdf126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Machine', 'Learning', 'can', 'play', 'a']),\n",
       " WordList(['Learning', 'can', 'play', 'a', 'pivotal']),\n",
       " WordList(['can', 'play', 'a', 'pivotal', 'role']),\n",
       " WordList(['play', 'a', 'pivotal', 'role', 'in']),\n",
       " WordList(['a', 'pivotal', 'role', 'in', 'a']),\n",
       " WordList(['pivotal', 'role', 'in', 'a', 'range']),\n",
       " WordList(['role', 'in', 'a', 'range', 'of']),\n",
       " WordList(['in', 'a', 'range', 'of', 'applications']),\n",
       " WordList(['a', 'range', 'of', 'applications', 'such']),\n",
       " WordList(['range', 'of', 'applications', 'such', 'as']),\n",
       " WordList(['of', 'applications', 'such', 'as', 'Deep']),\n",
       " WordList(['applications', 'such', 'as', 'Deep', 'Learning']),\n",
       " WordList(['such', 'as', 'Deep', 'Learning', 'Reinforcement']),\n",
       " WordList(['as', 'Deep', 'Learning', 'Reinforcement', 'Learning']),\n",
       " WordList(['Deep', 'Learning', 'Reinforcement', 'Learning', 'Natural']),\n",
       " WordList(['Learning', 'Reinforcement', 'Learning', 'Natural', 'Language']),\n",
       " WordList(['Reinforcement', 'Learning', 'Natural', 'Language', 'Processing']),\n",
       " WordList(['Learning', 'Natural', 'Language', 'Processing', 'etc']),\n",
       " WordList(['Natural', 'Language', 'Processing', 'etc', 'A']),\n",
       " WordList(['Language', 'Processing', 'etc', 'A', 'prime']),\n",
       " WordList(['Processing', 'etc', 'A', 'prime', 'example']),\n",
       " WordList(['etc', 'A', 'prime', 'example', 'of']),\n",
       " WordList(['A', 'prime', 'example', 'of', 'the']),\n",
       " WordList(['prime', 'example', 'of', 'the', 'application']),\n",
       " WordList(['example', 'of', 'the', 'application', 'of']),\n",
       " WordList(['of', 'the', 'application', 'of', 'machine']),\n",
       " WordList(['the', 'application', 'of', 'machine', 'learning']),\n",
       " WordList(['application', 'of', 'machine', 'learning', 'is']),\n",
       " WordList(['of', 'machine', 'learning', 'is', 'the']),\n",
       " WordList(['machine', 'learning', 'is', 'the', 'autonomous']),\n",
       " WordList(['learning', 'is', 'the', 'autonomous', 'vehicle']),\n",
       " WordList(['is', 'the', 'autonomous', 'vehicle', 'Sensors']),\n",
       " WordList(['the', 'autonomous', 'vehicle', 'Sensors', 'around']),\n",
       " WordList(['autonomous', 'vehicle', 'Sensors', 'around', 'the']),\n",
       " WordList(['vehicle', 'Sensors', 'around', 'the', 'vehicle']),\n",
       " WordList(['Sensors', 'around', 'the', 'vehicle', 'deliver']),\n",
       " WordList(['around', 'the', 'vehicle', 'deliver', 'thousands']),\n",
       " WordList(['the', 'vehicle', 'deliver', 'thousands', 'of']),\n",
       " WordList(['vehicle', 'deliver', 'thousands', 'of', 'data']),\n",
       " WordList(['deliver', 'thousands', 'of', 'data', 'points']),\n",
       " WordList(['thousands', 'of', 'data', 'points', 'which']),\n",
       " WordList(['of', 'data', 'points', 'which', 'are']),\n",
       " WordList(['data', 'points', 'which', 'are', 'analyzed']),\n",
       " WordList(['points', 'which', 'are', 'analyzed', 'and']),\n",
       " WordList(['which', 'are', 'analyzed', 'and', 'processed']),\n",
       " WordList(['are', 'analyzed', 'and', 'processed', 'to']),\n",
       " WordList(['analyzed', 'and', 'processed', 'to', 'move']),\n",
       " WordList(['and', 'processed', 'to', 'move', 'the']),\n",
       " WordList(['processed', 'to', 'move', 'the', 'vehicle']),\n",
       " WordList(['to', 'move', 'the', 'vehicle', 'toward']),\n",
       " WordList(['move', 'the', 'vehicle', 'toward', 'its']),\n",
       " WordList(['the', 'vehicle', 'toward', 'its', 'destination']),\n",
       " WordList(['vehicle', 'toward', 'its', 'destination', 'Collective']),\n",
       " WordList(['toward', 'its', 'destination', 'Collective', 'data']),\n",
       " WordList(['its', 'destination', 'Collective', 'data', 'from']),\n",
       " WordList(['destination', 'Collective', 'data', 'from', 'thousands']),\n",
       " WordList(['Collective', 'data', 'from', 'thousands', 'of']),\n",
       " WordList(['data', 'from', 'thousands', 'of', 'self-driving']),\n",
       " WordList(['from', 'thousands', 'of', 'self-driving', 'cars']),\n",
       " WordList(['thousands', 'of', 'self-driving', 'cars', 'can']),\n",
       " WordList(['of', 'self-driving', 'cars', 'can', 'be']),\n",
       " WordList(['self-driving', 'cars', 'can', 'be', 'used']),\n",
       " WordList(['cars', 'can', 'be', 'used', 'to']),\n",
       " WordList(['can', 'be', 'used', 'to', 'improve']),\n",
       " WordList(['be', 'used', 'to', 'improve', 'vehicle']),\n",
       " WordList(['used', 'to', 'improve', 'vehicle', 'safety']),\n",
       " WordList(['to', 'improve', 'vehicle', 'safety', 'and']),\n",
       " WordList(['improve', 'vehicle', 'safety', 'and', 'prevent']),\n",
       " WordList(['vehicle', 'safety', 'and', 'prevent', 'accidents'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "TextBlob(paragraph).ngrams(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044d1e5",
   "metadata": {},
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7428ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('sending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23c841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a139298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('send')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c7ad228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formula'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('formulae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe984fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('giving')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b104b8",
   "metadata": {},
   "source": [
    "### LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26cfd546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('sending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c196f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3462518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'giv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('giving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df91cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formula'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('formulae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db503b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5ab1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kniv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('knives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38ca0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ambigu'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "lst.stem('ambiguous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "364617d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44aef888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sending'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('sending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11b9301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'giving'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('giving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93d841e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formula'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('formulae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a290b",
   "metadata": {},
   "source": [
    "### STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e2b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de17d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b62175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae24ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'can', 'play', 'a', 'pivotal', 'role', 'in', 'a', 'range', 'of', 'applications', 'such', 'as', 'deep', 'learning', ',', 'reinforcement', 'learning', ',', 'natural', 'language', 'processing', ',', 'etc', '.', 'a', 'prime', 'example', 'of', 'the', 'application', 'of', 'machine', 'learning', 'is', 'the', 'autonomous', 'vehicle', '.', 'sensors', 'around', 'the', 'vehicle', 'deliver', 'thousands', 'of', 'data', 'points', 'which', 'are', 'analyzed', 'and', 'processed', 'to', 'move', 'the', 'vehicle', 'toward', 'its', 'destination', '.', 'collective', 'data', 'from', 'thousands', 'of', 'self-driving', 'cars', 'can', 'be', 'used', 'to', 'improve', 'vehicle', 'safety', 'and', 'prevent', 'accidents', '.']\n"
     ]
    }
   ],
   "source": [
    "paragraph1 = word_tokenize(paragraph.lower())\n",
    "print(paragraph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c855a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [x for x in paragraph1 if x not in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19cc2172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'play', 'pivotal', 'role', 'range', 'applications', 'deep', 'learning', ',', 'reinforcement', 'learning', ',', 'natural', 'language', 'processing', ',', 'etc', '.', 'prime', 'example', 'application', 'machine', 'learning', 'autonomous', 'vehicle', '.', 'sensors', 'around', 'vehicle', 'deliver', 'thousands', 'data', 'points', 'analyzed', 'processed', 'move', 'vehicle', 'toward', 'destination', '.', 'collective', 'data', 'thousands', 'self-driving', 'cars', 'used', 'improve', 'vehicle', 'safety', 'prevent', 'accidents', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "052976ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dc46dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: conda-script.py [-h] [-V] command ...\n",
      "conda-script.py: error: unrecognized arguments: wordcloud=1.2.1\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge wordcloud=1.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbe9bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d288336f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordcloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m all_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m stopwords])\n\u001b[1;32m----> 2\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mwordcloud\u001b[49m(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m, max_font_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m110\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(all_words)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordcloud' is not defined"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join([t for t in stopwords])\n",
    "wordcloud = wordcloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05218be0",
   "metadata": {},
   "source": [
    "# pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d0c5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dharniha V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5d2ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "692d6cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'NN')]\n",
      "[('Learning', 'VBG')]\n",
      "[('can', 'MD')]\n",
      "[('play', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('pivotal', 'NN')]\n",
      "[('role', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('range', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('applications', 'NNS')]\n",
      "[('such', 'JJ')]\n",
      "[('as', 'IN')]\n",
      "[('Deep', 'JJ')]\n",
      "[('Learning', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('Reinforcement', 'NN')]\n",
      "[('Learning', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('Natural', 'JJ')]\n",
      "[('Language', 'NN')]\n",
      "[('Processing', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('etc', 'NN')]\n",
      "[('.', '.')]\n",
      "[('A', 'DT')]\n",
      "[('prime', 'NN')]\n",
      "[('example', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('application', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('machine', 'NN')]\n",
      "[('learning', 'VBG')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('autonomous', 'JJ')]\n",
      "[('vehicle', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Sensors', 'NNS')]\n",
      "[('around', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('vehicle', 'NN')]\n",
      "[('deliver', 'NN')]\n",
      "[('thousands', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('data', 'NNS')]\n",
      "[('points', 'NNS')]\n",
      "[('which', 'WDT')]\n",
      "[('are', 'VBP')]\n",
      "[('analyzed', 'VBN')]\n",
      "[('and', 'CC')]\n",
      "[('processed', 'VBN')]\n",
      "[('to', 'TO')]\n",
      "[('move', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('vehicle', 'NN')]\n",
      "[('toward', 'IN')]\n",
      "[('its', 'PRP$')]\n",
      "[('destination', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Collective', 'JJ')]\n",
      "[('data', 'NNS')]\n",
      "[('from', 'IN')]\n",
      "[('thousands', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('self-driving', 'NN')]\n",
      "[('cars', 'NNS')]\n",
      "[('can', 'MD')]\n",
      "[('be', 'VB')]\n",
      "[('used', 'VBN')]\n",
      "[('to', 'TO')]\n",
      "[('improve', 'VB')]\n",
      "[('vehicle', 'NN')]\n",
      "[('safety', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('prevent', 'NN')]\n",
      "[('accidents', 'NNS')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in text2:\n",
    "  print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2867052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8f73178",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d09ab908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94dea772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accidents', 'analyzed', 'application', 'applications', 'around', 'autonomous', 'cars', 'collective', 'data', 'deep', 'deliver', 'destination', 'driving', 'etc', 'example', 'improve', 'language', 'learning', 'machine', 'move', 'natural', 'pivotal', 'play', 'points', 'prevent', 'prime', 'processed', 'processing', 'range', 'reinforcement', 'role', 'safety', 'self', 'sensors', 'thousands', 'toward', 'used', 'vehicle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dharniha V\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244730f3",
   "metadata": {},
   "source": [
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1988a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2d238a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter = Counter(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "884c1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_occur = Counter.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afaead00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 4),\n",
       " ('.', 4),\n",
       " ('vehicle', 4),\n",
       " (',', 3),\n",
       " ('machine', 2),\n",
       " ('thousands', 2),\n",
       " ('data', 2),\n",
       " ('play', 1),\n",
       " ('pivotal', 1),\n",
       " ('role', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccb3f3",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c2185d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "914e5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bfd84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "961474ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\Dharniha\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1a82e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "314720da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Machine/NN Learning/NNP)\n",
      "  can/MD\n",
      "  play/VB\n",
      "  a/DT\n",
      "  pivotal/JJ\n",
      "  role/NN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  range/NN\n",
      "  of/IN\n",
      "  applications/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  (PERSON Deep/NNP Learning/NNP)\n",
      "  ,/,\n",
      "  (ORGANIZATION Reinforcement/NNP Learning/NNP)\n",
      "  ,/,\n",
      "  (ORGANIZATION Natural/NNP Language/NNP Processing/NNP)\n",
      "  ,/,\n",
      "  etc/FW\n",
      "  ./.\n",
      "  A/DT\n",
      "  prime/JJ\n",
      "  example/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  application/NN\n",
      "  of/IN\n",
      "  machine/NN\n",
      "  learning/NN\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  autonomous/JJ\n",
      "  vehicle/NN\n",
      "  ./.\n",
      "  Sensors/NNS\n",
      "  around/IN\n",
      "  the/DT\n",
      "  vehicle/NN\n",
      "  deliver/VB\n",
      "  thousands/NNS\n",
      "  of/IN\n",
      "  data/NNS\n",
      "  points/NNS\n",
      "  which/WDT\n",
      "  are/VBP\n",
      "  analyzed/VBN\n",
      "  and/CC\n",
      "  processed/VBN\n",
      "  to/TO\n",
      "  move/VB\n",
      "  the/DT\n",
      "  vehicle/NN\n",
      "  toward/IN\n",
      "  its/PRP$\n",
      "  destination/NN\n",
      "  ./.\n",
      "  Collective/JJ\n",
      "  data/NNS\n",
      "  from/IN\n",
      "  thousands/NNS\n",
      "  of/IN\n",
      "  self-driving/JJ\n",
      "  cars/NNS\n",
      "  can/MD\n",
      "  be/VB\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  improve/VB\n",
      "  vehicle/NN\n",
      "  safety/NN\n",
      "  and/CC\n",
      "  prevent/NN\n",
      "  accidents/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf134bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
